# python爬虫

## 1.Fzu文件夹（福大教务处爬取）

### 1.福专主页爬取.py

​	通过    `requests`    方法获取到福大教务处的页面地址，先进行解析获取每一页的数据，再通过拼凑的方法得到每一页的 url 进行读取。

> [!CAUTION]
>
> **分别获取到了六个点**

- 通知人

- 标题

- 日期

- 详情页面链接

- 附件下载总次数 （没想到很好的将附件分离出去的操作 故直接进行累加了）

- 附件名字（中间以   `     `   进行分离）

  

![image-20241114191018350](C:\Users\Slexy\AppData\Roaming\Typora\typora-user-images\image-20241114191018350.png)



> #### ***<u>一些奇怪的注意事项</u>***

1. 建议导入 `time` 库可能是我抓取附件下载地址的时候重复 使用了 `get` 导致循环过多可能被ban







------

### 2.attachment.py

​	这个文件作为自建的模块用来处理附件，通过导入页面的详细地址，对附件的名字进行抓包判断，若没有附件直接返回空列表和0（表明没有附件）



> #### *<u>**一些奇怪的注意事项**</u>*

1. 对附件名字的处理，这边需要特别点名一些奇怪的党政文件，正常文件的转换为中文输出可以这样子写

   

   `file_name.encode('iso-8859-1').decode('utf-8')`

   

   可是这类文件不一样他需要进行进行这样子的转换才能输出中文

   

   `file_name.encode('utf-8').decode('utf-8')`

   

   而且使用第一类对于这类文件会发生  `UnicodeEncodeError` 的报错

   **所以我的解决方案就是使用  try - except 把报错提取 然后进行更改**

   ```python
           try:
               # 尝试执行的代码
               file_name2.append(file_name1.encode('iso-8859-1').decode('utf-8'))
           except UnicodeEncodeError:
               # 退而求其次
               file_name2.append(file_name1.encode('utf-8').decode('utf-8'))
               print("  **文件  ")
   ```

   

   

2.  对于提取下载次数问题的处理，我发现直接用xpath无法提取到下载的次数，再尝试几次无果后，我发现这个下载次数再一个get命令下的返回包中，而且每一个都是相同的，只是id不同，故而我就先将每一个的id提取出来，拼凑成一个url再调用另一个模块 **（get_download_numbers.py）** 对这个下载次数进行处理。






### 3.get_download_numbers.py

​	这个模块通过提取get命令返回的值进行提取下载次数的操作



> #### *<u>**一些奇怪的注意事项**</u>*

1. 其实也没啥，就是获取下来的是个str类型的数据，需要稍微对数据进行一些处理，得到最后的数字类型的下载次数







------

### 4.csv_making.py

​	这个模块主要有两个功能，创建一个csv供写入使用和传入数据写入csv文件中



> #### *<u>**一些奇怪的注意事项**</u>*

1. 没啥注意的主要是复习了一些有关于文件oi的问题